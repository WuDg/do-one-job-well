# 概览

[文档地址](https://ci.apache.org/projects/flink/flink-docs-release-1.12/zh/learn-flink)

## 目标&涵盖范围

对Apache Flink基本概念介绍

实现可扩展并行度的ETL‘数据分析以及实践驱动的流式应用程序

对Flink API中的状态管理和时间进行介绍

本章将学习到以下内容：

* 如何实现流数据处理管道（pipelines）
* Flink如何管理状态以及为何需要管理状态
* 如何使用事件时间（event time）来一致并准确地进行计算分析
* 如何在源源不断的数据流上构建时间驱动的应用程序
* Flink如何提供具有精确一次（exactly-once）计算语义的可容错、有状态流处理

四个概念：源源不断的流式数据处理、事件事件、有状态流处理和状态快照



## 流处理

在自然环境中，数据的产生原本就是流式的。

分析数据：`有界流`和`无界流`

![Bounded and unbounded streams](../../../../做好一件事/docs/images/bounded-unbounded.png)

`批处理`是有界数据流处理的范例

可以选择在计算结果输出之前输入整个数据集，可以对整个数据集的数据进行排序、统计或汇总计算后再输出结果

`流处理`正相反，涉及无界数据流。理论上来说，它的数据输入永远不会结束，因此程序必须持续不断地对到达的数据进行处理

在Flink中，应用程序由用户自定义`算子`转换而来的`流式` `dataflows` 所组成。这些流式 dataflows 形成了有向图，以一个或多个`源`(source)开始，并以一个或多个`汇`（sink）结束



![A DataStream program, and its dataflow.](../../../../做好一件事/docs/images/program_dataflow.svg)

Flink应用程序可以消费来自消息队列或分布式日志的这类流式数据源（如Apache Kafka或Kinesis）的实时数据，也可以从各种的数据源中消费有界的历史数据。同样，Flink应用程序生成的结果流也可以发送到各种数据汇中

![Flink application with sources and sinks](../../../../做好一件事/docs/images/flink-application-sources-sinks.png)

### 并行 Dataflows

Flink程序本质上是分布式并行程序。在程序执行期间，一个流有一个或多个流分区（Stream Partition），每个算子有一个或多个算子子任务。每个子任务彼此独立，并在不同的线程中运行，或在不同的计算机或容器中运行

算法子任务数就是其对应算子的并行度。在同一程序中，不同算子也可能具有不同的并行度

![A parallel dataflow](../../../../做好一件事/docs/images/parallel_dataflow.svg)